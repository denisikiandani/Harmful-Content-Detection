{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40eeaa-70c4-4d66-ae5c-098b7c3b8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAKE GOOGLE SPEECH API V1\n",
    "\n",
    "from google.cloud import speech\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "def transcribe_file(speech_file: str) -> speech.RecognizeResponse:\n",
    "    \"\"\"Transcribe the given audio file.\"\"\"\n",
    "    credentials = service_account.Credentials.from_service_account_file('gcloud_apikey.json')\n",
    "    client = speech.SpeechClient(credentials=credentials)\n",
    "\n",
    "    with open(speech_file, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.MP3,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "        model=\"video\",\n",
    "    )\n",
    "\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "    # Each result is for a consecutive portion of the audio. Iterate through\n",
    "    # them to get the transcripts for the entire audio file.\n",
    "    for result in response.results:\n",
    "        # The first alternative is the most likely one for this portion.\n",
    "        print(f\"Transcript: {result.alternatives[0].transcript}\")\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa974788f75f869",
   "metadata": {},
   "source": [
    "https://dev.to/puritye/how-to-plot-an-audio-file-using-matplotlib-pbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6331ae3a-099f-47c1-9c84-c6492ebab51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/speech-to-text/docs/transcribe-streaming-audio\n",
    "\n",
    "# pip install google-cloud-speech\n",
    "# pip install pyaudio\n",
    "\n",
    "import queue\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from google.cloud import speech\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import pyaudio\n",
    "\n",
    "# Audio recording parameters\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE / 10)  # 100ms\n",
    "\n",
    "\n",
    "class MicrophoneStream:\n",
    "    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n",
    "\n",
    "    def __init__(self: object, rate: int = RATE, chunk: int = CHUNK) -> None:\n",
    "        \"\"\"The audio -- and generator -- is guaranteed to be on the main thread.\"\"\"\n",
    "        self._rate = rate\n",
    "        self._chunk = chunk\n",
    "\n",
    "        # Create a thread-safe buffer of audio data\n",
    "        self._buff = queue.Queue()\n",
    "        self.closed = True\n",
    "\n",
    "    def __enter__(self: object) -> object:\n",
    "        self._audio_interface = pyaudio.PyAudio()\n",
    "        self._audio_stream = self._audio_interface.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            # The API currently only supports 1-channel (mono) audio\n",
    "            # https://goo.gl/z757pE\n",
    "            channels=1,\n",
    "            rate=self._rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self._chunk,\n",
    "            # Run the audio stream asynchronously to fill the buffer object.\n",
    "            # This is necessary so that the input device's buffer doesn't\n",
    "            # overflow while the calling thread makes network requests, etc.\n",
    "            stream_callback=self._fill_buffer,\n",
    "        )\n",
    "\n",
    "        self.closed = False\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(\n",
    "        self: object,\n",
    "        type: object,\n",
    "        value: object,\n",
    "        traceback: object,\n",
    "    ) -> None:\n",
    "        \"\"\"Closes the stream, regardless of whether the connection was lost or not.\"\"\"\n",
    "        self._audio_stream.stop_stream()\n",
    "        self._audio_stream.close()\n",
    "        self.closed = True\n",
    "        # Signal the generator to terminate so that the client's\n",
    "        # streaming_recognize method will not block the process termination.\n",
    "        self._buff.put(None)\n",
    "        self._audio_interface.terminate()\n",
    "\n",
    "    def _fill_buffer(\n",
    "        self: object,\n",
    "        in_data: object,\n",
    "        frame_count: int,\n",
    "        time_info: object,\n",
    "        status_flags: object,\n",
    "    ) -> object:\n",
    "        \"\"\"Continuously collect data from the audio stream, into the buffer.\n",
    "\n",
    "        Args:\n",
    "            in_data: The audio data as a bytes object\n",
    "            frame_count: The number of frames captured\n",
    "            time_info: The time information\n",
    "            status_flags: The status flags\n",
    "\n",
    "        Returns:\n",
    "            The audio data as a bytes object\n",
    "        \"\"\"\n",
    "        self._buff.put(in_data)\n",
    "        return None, pyaudio.paContinue\n",
    "\n",
    "    def generator(self: object) -> object:\n",
    "        \"\"\"Generates audio chunks from the stream of audio data in chunks.\n",
    "\n",
    "        Args:\n",
    "            self: The MicrophoneStream object\n",
    "\n",
    "        Returns:\n",
    "            A generator that outputs audio chunks.\n",
    "        \"\"\"\n",
    "        while not self.closed:\n",
    "            # Use a blocking get() to ensure there's at least one chunk of\n",
    "            # data, and stop iteration if the chunk is None, indicating the\n",
    "            # end of the audio stream.\n",
    "            chunk = self._buff.get()\n",
    "            if chunk is None:\n",
    "                return\n",
    "            data = [chunk]\n",
    "\n",
    "            # Now consume whatever other data's still buffered.\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = self._buff.get(block=False)\n",
    "                    if chunk is None:\n",
    "                        return\n",
    "                    data.append(chunk)\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "\n",
    "            yield b\"\".join(data)\n",
    "\n",
    "\n",
    "def listen_print_loop(responses: object) -> str:\n",
    "    \"\"\"Iterates through server responses and prints them.\n",
    "\n",
    "    The responses passed is a generator that will block until a response\n",
    "    is provided by the server.\n",
    "\n",
    "    Each response may contain multiple results, and each result may contain\n",
    "    multiple alternatives; for details, see https://goo.gl/tjCPAU.  Here we\n",
    "    print only the transcription for the top alternative of the top result.\n",
    "\n",
    "    In this case, responses are provided for interim results as well. If the\n",
    "    response is an interim one, print a line feed at the end of it, to allow\n",
    "    the next result to overwrite it, until the response is a final one. For the\n",
    "    final one, print a newline to preserve the finalized transcription.\n",
    "\n",
    "    Args:\n",
    "        responses: List of server responses\n",
    "\n",
    "    Returns:\n",
    "        The transcribed text.\n",
    "    \"\"\"\n",
    "    num_chars_printed = 0\n",
    "    for response in responses:\n",
    "        if not response.results:\n",
    "            continue\n",
    "\n",
    "        # The `results` list is consecutive. For streaming, we only care about\n",
    "        # the first result being considered, since once it's `is_final`, it\n",
    "        # moves on to considering the next utterance.\n",
    "        result = response.results[0]\n",
    "        if not result.alternatives:\n",
    "            continue\n",
    "\n",
    "        # Display the transcription of the top alternative.\n",
    "        transcript = result.alternatives[0].transcript\n",
    "\n",
    "        # Display interim results, but with a carriage return at the end of the\n",
    "        # line, so subsequent lines will overwrite them.\n",
    "        #\n",
    "        # If the previous result was longer than this one, we need to print\n",
    "        # some extra spaces to overwrite the previous result\n",
    "        overwrite_chars = \" \" * (num_chars_printed - len(transcript))\n",
    "\n",
    "        if not result.is_final:\n",
    "            sys.stdout.write(transcript + overwrite_chars + \"\\r\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            num_chars_printed = len(transcript)\n",
    "\n",
    "        else:\n",
    "            print(transcript + overwrite_chars)\n",
    "\n",
    "            # Exit recognition if any of the transcribed phrases could be\n",
    "            # one of our keywords.\n",
    "            if re.search(r\"\\b(exit|quit)\\b\", transcript, re.I):\n",
    "                print(\"Exiting..\")\n",
    "                break\n",
    "\n",
    "            num_chars_printed = 0\n",
    "\n",
    "    return transcript\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Transcribe speech from audio file.\"\"\"\n",
    "    # See http://g.co/cloud/speech/docs/languages\n",
    "    # for a list of supported languages.\n",
    "    language_code = \"id-ID\"  # a BCP-47 language tag\n",
    "    language_code = \"en-US\"  # a BCP-47 language tag\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file('gcloud_apikey.json')\n",
    "    client = speech.SpeechClient(credentials=credentials)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=language_code,\n",
    "    )\n",
    "\n",
    "    streaming_config = speech.StreamingRecognitionConfig(\n",
    "        config=config, interim_results=True, single_utterance=True\n",
    "    )\n",
    "\n",
    "    with MicrophoneStream(RATE, CHUNK) as stream:\n",
    "        audio_generator = stream.generator()\n",
    "        requests = (\n",
    "            speech.StreamingRecognizeRequest(audio_content=content)\n",
    "            for content in audio_generator\n",
    "        )\n",
    "\n",
    "        responses = client.streaming_recognize(streaming_config, requests)\n",
    "\n",
    "        # Now, put the transcription responses to use.\n",
    "        listen_print_loop(responses)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0767d-6168-4255-8844-173b542389e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Library\n",
    "import json\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# Menyimpang hasil transkrip ke dalam dict\n",
    "hasil_transkrip_dict = MessageToDict(hasil_transkrip._pb)\n",
    "\n",
    "# Menyimpan hasil transkrip ke dalam file JSON\n",
    "with open('hasil_transkrip.json', 'w') as file:\n",
    "    json.dump(hasil_transkrip_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad3021b959c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26949dbd8769ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = wave.open('Aurora.wav', 'rb')\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c91acfed960a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters:', obj.getparams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f9b2f5a82f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_freq = obj.getframerate()\n",
    "sample_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c72b3ccc2bfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = obj.getnframes()\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62518f41e6830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_wave = obj.readframes(n_samples)\n",
    "signal_wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1111000771f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = n_samples/sample_freq\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b01c8ac1e67b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_array = np.frombuffer(signal_wave, dtype=np.int16)\n",
    "signal_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5786d7e6d05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.linspace(0, duration, num=n_samples)\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30c53ed7d1d7b3",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/plotting-various-sounds-on-graphs-using-python-and-matplotlib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253720c8bdbbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import wave, sys \n",
    "\n",
    "# shows the sound waves \n",
    "def visualize(path: str): \n",
    "    \n",
    "    # reading the audio file \n",
    "    raw = wave.open(path) \n",
    "    # reads all the frames  \n",
    "    # -1 indicates all or max frames \n",
    "    signal = raw.readframes(-1) \n",
    "    signal = np.frombuffer(signal, dtype =\"int16\") \n",
    "\n",
    "    # gets the frame rate \n",
    "    f_rate = raw.getframerate() \n",
    "\n",
    "    # to Plot the x-axis in seconds  \n",
    "    # you need get the frame rate  \n",
    "    # and divide by size of your signal \n",
    "    # to create a Time Vector  \n",
    "    # spaced linearly with the size  \n",
    "    # of the audio file \n",
    "    time = np.linspace( \n",
    "        0, # start \n",
    "        len(signal) / f_rate, \n",
    "        num = len(signal) \n",
    "    ) \n",
    "\n",
    "    # using matplotlib to plot \n",
    "    # creates a new figure \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.figure(1) \n",
    "\n",
    "    # title of the plot \n",
    "    plt.title(\"Sound Wave\") \n",
    "\n",
    "    # label of x-axis \n",
    "    plt.xlabel(\"Time\") \n",
    "\n",
    "    # actual plotting \n",
    "    plt.plot(time, signal) \n",
    "\n",
    "    # shows the plot  \n",
    "    # in new window \n",
    "    plt.show() \n",
    "\n",
    "    # you can also save \n",
    "    # the plot using \n",
    "    # plt.savefig('filename') \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    # gets the command line Value \n",
    "    path = './Aurora.wav'\n",
    "\n",
    "    visualize(path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
