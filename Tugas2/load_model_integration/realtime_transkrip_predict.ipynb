{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38785f74f7e6c185",
   "metadata": {},
   "source": [
    "# INITIALIZE MODEL PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e37bfc85eb49012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T02:09:48.994955Z",
     "start_time": "2024-05-22T02:09:43.619540Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weight count mismatch for layer #1 (named bidirectional in the current model, bidirectional in the save file). Layer expects 9 weight(s). Received 6 saved weight(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load Model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../load_model_integration/toxic-v1.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Lables Predict\u001b[39;00m\n\u001b[0;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoxic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msevere_toxic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobscene\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentity_hate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\scoop\\apps\\python311\\current\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    177\u001b[0m         filepath,\n\u001b[0;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m     )\n",
      "File \u001b[1;32m~\\scoop\\apps\\python311\\current\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:138\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[0;32m    134\u001b[0m         model_config, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     \u001b[43mload_weights_from_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcompile\u001b[39m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# instantiate optimizer\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     training_config \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\scoop\\apps\\python311\\current\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:369\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    367\u001b[0m weight_values \u001b[38;5;241m=\u001b[39m load_subset_weights_from_hdf5_group(g)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weight_values) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(symbolic_weights):\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight count mismatch for layer #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe current model, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the save file). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(symbolic_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weight(s). Received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(weight_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved weight(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m     )\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_v, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(symbolic_weights, weight_values):\n\u001b[0;32m    376\u001b[0m     ref_v\u001b[38;5;241m.\u001b[39massign(val)\n",
      "\u001b[1;31mValueError\u001b[0m: Weight count mismatch for layer #1 (named bidirectional in the current model, bidirectional in the save file). Layer expects 9 weight(s). Received 6 saved weight(s)"
     ]
    }
   ],
   "source": [
    "# Import Library\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization  # tokenization|\n",
    "import pickle\n",
    "\n",
    "# Load Model\n",
    "model = tf.keras.models.load_model('../load_model_integration/toxic-v1.h5')\n",
    "\n",
    "# Lables Predict\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "# toxic == toxic\n",
    "# sever_toxic == toxic_parah\n",
    "# obscene == cabul\n",
    "# threat == ancaman\n",
    "# insult == menyinggung\n",
    "# indentity_hate == benci personal\n",
    "\n",
    "# Import Vectorizer\n",
    "with open('vectorizer_config.pkl', 'rb') as f:\n",
    "    vectorizer_config = pickle.load(f)\n",
    "with open('vectorizer_vocab.pkl', 'rb') as f:\n",
    "    vectorizer_vocab = pickle.load(f)\n",
    "\n",
    "# Set Vectorizer\n",
    "vectorizer = TextVectorization.from_config(vectorizer_config)\n",
    "vectorizer.set_vocabulary(vectorizer_vocab)\n",
    "\n",
    "\n",
    "# Function predict threat dan hate pada teks\n",
    "def predict(teks):\n",
    "    # Input data teks\n",
    "    input_data = teks\n",
    "    # Membuat vektor setiap teks masukan dalam daftar\n",
    "    vectorized_texts = [vectorizer(text) for text in input_data]\n",
    "    # Pad urutan dengan panjang yang sama\n",
    "    padded_texts = tf.keras.preprocessing.sequence.pad_sequences(vectorized_texts, maxlen=1800)\n",
    "    # Melakukan prediksi\n",
    "    predictions = model.predict(padded_texts)\n",
    "    binary_predictions = (predictions > 0.5).astype(int)\n",
    "    # Membuat buffer untuk menyimpan output\n",
    "    output_buffer = io.StringIO()\n",
    "    label_index = {label: [] for label in labels}\n",
    "    total_predictions = binary_predictions.sum(axis=0)\n",
    "    for i, (prediction, text) in enumerate(zip(binary_predictions, input_data)):\n",
    "        # Menulis ke buffer alih-alih mencetak langsung\n",
    "        for j, (label, pred) in enumerate(zip(labels, prediction)):\n",
    "            if pred == 1:\n",
    "                output_buffer.write(f\"Text: {text}\\n\")\n",
    "                output_buffer.write(f\"Prediction: {label} Value: {predictions[i][j]}\\n\")\n",
    "                label_index[label].append(i)  # Menyimpan indeks di mana label diprediksi sebagai 1\n",
    "        output_buffer.write(\"\\n\")\n",
    "    for label, total in zip(labels, total_predictions):\n",
    "        output_buffer.write(f\"Total {label} predictions: {total}\\n\")\n",
    "        output_buffer.write(f\"Index Kalimat yang terdeteksi {label}: {label_index[label]}\\n\")\n",
    "        output_buffer.write(\"\\n\")\n",
    "    # Mendapatkan semua output sebagai string\n",
    "    output_string = output_buffer.getvalue()\n",
    "    # Jangan lupa untuk menutup buffer setelah selesai\n",
    "    output_buffer.close()\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605562cef3cc0887",
   "metadata": {},
   "source": [
    "# INITIALIZE SPEECH API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T02:09:48.995960Z",
     "start_time": "2024-05-22T02:09:48.995960Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import Library\n",
    "from google.cloud.speech_v2 import SpeechClient\n",
    "from google.cloud.speech_v2.types import cloud_speech as cloud_speech_types\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "def transkrip_dan_predict(project_id, api_key, audio_file):\n",
    "\n",
    "    # Instantiates a client\n",
    "    credentials = service_account.Credentials.from_service_account_file(api_key)\n",
    "    client = SpeechClient(credentials=credentials)\n",
    "\n",
    "    # Reads a file as bytes\n",
    "    with open(audio_file, \"rb\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    sample_rate = 16000  # Misalkan sample rate adalah 16000 Hz (16kHz)\n",
    "    desired_duration = 1  # Durasi yang diinginkan untuk setiap chunk dalam detik\n",
    "\n",
    "    # In practice, stream should be a generator yielding chunks of audio data\n",
    "    # chunk_length = len(content) // 1000\n",
    "    chunk_length = sample_rate * desired_duration\n",
    "    stream = [\n",
    "        content[start: start + chunk_length]\n",
    "        for start in range(0, len(content), chunk_length)\n",
    "    ]\n",
    "    audio_requests = (\n",
    "        cloud_speech_types.StreamingRecognizeRequest(audio=audio) for audio in stream\n",
    "    )\n",
    "\n",
    "    recognition_config = cloud_speech_types.RecognitionConfig(\n",
    "        auto_decoding_config=cloud_speech_types.AutoDetectDecodingConfig(),\n",
    "        language_codes=[\"en-US\"],\n",
    "        model=\"long\",\n",
    "    )\n",
    "    streaming_config = cloud_speech_types.StreamingRecognitionConfig(\n",
    "        config=recognition_config\n",
    "    )\n",
    "    config_request = cloud_speech_types.StreamingRecognizeRequest(\n",
    "        recognizer=f\"projects/{project_id}/locations/global/recognizers/_\",\n",
    "        streaming_config=streaming_config,\n",
    "    )\n",
    "\n",
    "    def requests(config: cloud_speech_types.RecognitionConfig, audio: list) -> list:\n",
    "        yield config\n",
    "        yield from audio\n",
    "\n",
    "    # Transcribes the audio into text\n",
    "    responses_iterator = client.streaming_recognize(\n",
    "        requests=requests(config_request, audio_requests)\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    list_transkrip = []\n",
    "\n",
    "    for response in responses_iterator:\n",
    "        responses.append(response)\n",
    "        for result in response.results:\n",
    "            predict([str(result.alternatives[0].transcript)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81376735a37ed162",
   "metadata": {},
   "source": [
    "# PROGRAM UTAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e61fea10aaadd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transkrip_dan_predict(project_id=\"data-science-programming-ti24\",\n",
    "                        api_key=\"./gcloud_apikey.json\",\n",
    "                        audio_file=\"./Donald Trump.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
